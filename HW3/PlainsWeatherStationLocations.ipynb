{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS + Spark: Exploration of NOAA Weather Data\n",
    "\n",
    "This notebook demonstrates how to format and plot data with the *leaflet* class, which uses ipyleaflet for data visualization on an interactive map.\n",
    "\n",
    "## Import libraries.\n",
    "The main utilities are the *leaflet* class which plots data and some functions for pulling data from the S3 bucket.\n",
    "The utilities library also contains other functions that the professor uses throughout the notebooks, but they are not necessarily used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:46:14.330886Z",
     "start_time": "2020-05-20T03:46:13.742043Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import sys\n",
    "import numpy as np\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from lib import *\n",
    "from pyspark.sql import *\n",
    "from lib.utils import *\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new SparkContext\n",
    "\n",
    "Set path for the data directory (you will need to manually create ../DataHW3 if it doesn't already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:56:26.394995Z",
     "start_time": "2020-05-20T03:56:25.769879Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ../DataHW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:56:31.756909Z",
     "start_time": "2020-05-20T03:56:28.582364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new spark and sql context\n",
    "sc = create_sc(pyFiles=['lib/numpy_pack.py','lib/spark_PCA.py','lib/computeStatistics.py'])\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Create the names of the files/directories we're working with\n",
    "\n",
    "data_dir = '../DataHW3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if we've already downloaded this file\n",
    "\n",
    "If we have not, we'll need to grab it from the course's S3 bucket\n",
    "\n",
    "Here are the states that we will look at: <br>\n",
    "**Northern plains**: North Dakota, South Dakota, Minnesota, Iowa, Nebraska\n",
    "<br>**Southern plains**: Texas, Oklahoma, Kansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:59:23.263915Z",
     "start_time": "2020-05-20T03:56:38.078044Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../DataHW3'\n",
    "\n",
    "if not path.exists(data_dir + '/' + 'stations.parquet'):\n",
    "    getStations()\n",
    "\n",
    "    \n",
    "states = ['ND', 'SD', 'MN', 'IA', 'NE', 'TX', 'OK', 'KS']\n",
    "\n",
    "for s in states:\n",
    "    \n",
    "    parquet = s + '.parquet'\n",
    "    tarname = s + '.tgz'\n",
    "    \n",
    "    if not path.exists(data_dir + '/' + parquet):\n",
    "\n",
    "        # pull the weather data for a particular state from the MAS-DSE S3 bucket\n",
    "        getStateData(s, data_dir, tarname, parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T17:38:42.826784Z",
     "start_time": "2020-05-07T17:38:42.231170Z"
    }
   },
   "source": [
    "## Create a SQL context for our data\n",
    "\n",
    "... and perform some sample queries to see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:59:30.489593Z",
     "start_time": "2020-05-20T03:59:23.284228Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform some sample queries to see what our data looks like\n",
    "\n",
    "state = 'ND'\n",
    "tarname = state + '.tgz'\n",
    "parquet = state + '.parquet'\n",
    "parquet_path = data_dir + '/' + parquet\n",
    "\n",
    "stations_df = sqlContext.read.parquet(data_dir+'/stations.parquet')\n",
    "sqlContext.registerDataFrameAsTable(stations_df, 'stations_table')\n",
    "\n",
    "\n",
    "# create the sparksql context\n",
    "df = sqlContext.read.parquet(parquet_path)\n",
    "sqlContext.registerDataFrameAsTable(df,'table')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Number of stations in ND that have measurement\n",
    "Query=\"\"\"\n",
    "SELECT DISTINCT(Measurement)\n",
    "FROM table\n",
    "WHERE STATE=='ND'\n",
    "\"\"\"\n",
    "counts=sqlContext.sql(Query)\n",
    "counts.show()\n",
    "\n",
    "# First 5 measurements from the weather table. Note that it looks like 1 measurement is really = 1 year of measurements for that measurement type\n",
    "Query = \"\"\"\n",
    "SELECT *\n",
    "FROM table\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "all_features = sqlContext.sql(Query)\n",
    "all_features.show()\n",
    "\n",
    "# Show the ND stations with the top number of measurements\n",
    "Query = \"\"\"\n",
    "SELECT Station, Measurement, Values, longitude, latitude\n",
    "FROM table\n",
    "WHERE Measurement=='PRCP'\n",
    "\"\"\"\n",
    "weather_query = sqlContext.sql(Query)\n",
    "weather_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:37:45.847903Z",
     "start_time": "2020-05-20T04:37:45.819766Z"
    }
   },
   "source": [
    "Below are the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T04:44:13.092786Z",
     "start_time": "2020-05-20T04:43:04.841162Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# featureStr = \"\\'SNWD\\'\"\n",
    "\n",
    "# dfs = []\n",
    "# # create the sparksql context\n",
    "# for s in states:\n",
    "#     parquet = s + '.parquet'\n",
    "#     parquet_path = data_dir + '/' + parquet\n",
    "#     df = sqlContext.read.parquet(parquet_path)\n",
    "#     sqlContext.registerDataFrameAsTable(df,f'table_{s}')\n",
    "    \n",
    "#     Query = f\"\"\"\n",
    "#     SELECT Year, Station, state, Measurement, Values\n",
    "#     FROM table_{s}\n",
    "#     WHERE Measurement=={featureStr} and Year > 1969 and Year < 2010\n",
    "#     \"\"\"\n",
    "#     weather_query = sqlContext.sql(Query)\n",
    "#     rdd2 = weather_query.rdd.map(lambda x: replaceSNWD(x, 'Values'))\n",
    "#     df2 = sqlContext.createDataFrame(rdd2).toPandas()\n",
    "#     dfs.append(df2)\n",
    "\n",
    "# all_states_SNWD = pd.concat(dfs)\n",
    "# all_states_SNWD.to_excel('SNWD-All-States.xlsx')\n",
    "\n",
    "\n",
    "# featureStr = \"\\'SNOW\\'\"\n",
    "\n",
    "# dfs = []\n",
    "# # create the sparksql context\n",
    "# for s in states:\n",
    "#     parquet = s + '.parquet'\n",
    "#     parquet_path = data_dir + '/' + parquet\n",
    "#     df = sqlContext.read.parquet(parquet_path)\n",
    "#     sqlContext.registerDataFrameAsTable(df,f'table_{s}')\n",
    "    \n",
    "#     Query = f\"\"\"\n",
    "#     SELECT Year, Station, state, Measurement, Values\n",
    "#     FROM table_{s}\n",
    "#     WHERE Measurement=={featureStr} and Year > 1969 and Year < 2010\n",
    "#     \"\"\"\n",
    "#     weather_query = sqlContext.sql(Query)\n",
    "#     rdd2 = weather_query.rdd.map(lambda x: replaceSNWD(x, 'Values'))\n",
    "#     df2 = sqlContext.createDataFrame(rdd2).toPandas()\n",
    "#     dfs.append(df2)\n",
    "\n",
    "# all_states_SNOW = pd.concat(dfs)\n",
    "# all_states_SNOW.to_excel('SNOW-All-States.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of reducing the Values feature to a single value using a UDF (user defined function) from utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:09:29.762046Z",
     "start_time": "2020-05-14T23:09:20.697892Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the query that get's performed inside leaflet class, but Values have been cleaned\n",
    "df = sqlContext.read.parquet(parquet_path)\n",
    "sqlContext.registerDataFrameAsTable(df,'table')\n",
    "Query = \"\"\"\n",
    "SELECT Station, Measurement, Values, longitude, latitude\n",
    "FROM table\n",
    "WHERE Measurement=='SNWD'\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "weather_query = sqlContext.sql(Query)\n",
    "\n",
    "rdd3 = weather_query.rdd.map(lambda x: replaceSNWD(x, 'Values'))\n",
    "df2 = sqlContext.createDataFrame(rdd3)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T05:00:03.506049Z",
     "start_time": "2020-05-13T05:00:03.496250Z"
    }
   },
   "source": [
    "# Data visualization\n",
    "\n",
    "Depending on how many states' data we are plotting this could take a little while. The northern and souther plains combined took around ~5 min. \n",
    "\n",
    "Nevertheless, we can see the average snow depth during the first three calendar months of the year for these states. \n",
    "\n",
    "Note: I only included \"ND\" and \"SD\" for demonstration purposes, but it works with all of the states\n",
    "\n",
    "First initialize the leaflet with sqlContext, and the feature represented as a string.\n",
    "Then proceed to add states data to the leaflet class by the .add(dataframe) method. **The dataframe should have the format of the query below**.\n",
    "Once everything has been added, you can run plot_all(). This is the step that will take a while for longer data sets\n",
    "To show the map, run plotter.m, and to show the legend, run plotter.color_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:09:45.204842Z",
     "start_time": "2020-05-14T23:09:29.767200Z"
    }
   },
   "outputs": [],
   "source": [
    "featureStr = \"\\'SNWD\\'\"\n",
    "\n",
    "plotter = leaflet_old(sqlContext, featureStr)\n",
    "\n",
    "# create the sparksql context\n",
    "for s in [\"TX\"]:\n",
    "    parquet = s + '.parquet'\n",
    "    parquet_path = data_dir + '/' + parquet\n",
    "    df = sqlContext.read.parquet(parquet_path)\n",
    "    sqlContext.registerDataFrameAsTable(df,f'table_{s}')\n",
    "    \n",
    "    Query = f\"\"\"\n",
    "    SELECT Station, Measurement, Values, longitude, latitude\n",
    "    FROM table_{s}\n",
    "    WHERE Measurement=={featureStr}\n",
    "    \"\"\"\n",
    "    weather_query = sqlContext.sql(Query)\n",
    "    rdd2 = weather_query.rdd.map(lambda x: replaceSNWD(x, 'Values'))\n",
    "    df2 = sqlContext.createDataFrame(rdd2)\n",
    "    \n",
    "    \n",
    "    plotter.add(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:10:07.347412Z",
     "start_time": "2020-05-14T23:09:45.219855Z"
    }
   },
   "outputs": [],
   "source": [
    "plotter.plot_all()\n",
    "plotter.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:10:07.537038Z",
     "start_time": "2020-05-14T23:10:07.353468Z"
    }
   },
   "outputs": [],
   "source": [
    "plotter.color_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T23:10:07.666572Z",
     "start_time": "2020-05-14T23:10:07.539861Z"
    }
   },
   "outputs": [],
   "source": [
    "# class leaflet:\n",
    "#     \"\"\"\n",
    "#     Plots circles on a map (one per station) whose size is proportional to the number of feature measurements, and whose color is equal to their aggregated value (min, max, avg)  \n",
    "    \n",
    "#     :param featureStr: the feature you'd like to visualize given as a string, eg. 'SNWD'\n",
    "#     :param aggregateType: 'avg', 'min', 'max'\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, sqlctxt, featureStr):\n",
    "#         self.feat = featureStr\n",
    "#         self.pdfMaster = None\n",
    "#         self.sqlctxt = sqlctxt\n",
    "        \n",
    "#         self.maxLong = None\n",
    "#         self.minLong = None\n",
    "#         self.maxLat = None\n",
    "#         self.minLat = None\n",
    "        \n",
    "#         self.minAggVal = None\n",
    "#         self.maxAggVal = None\n",
    "        \n",
    "        \n",
    "#         self.cmap = plt.get_cmap('jet') \n",
    "#         self.m = None\n",
    "        \n",
    "#     def add(self, dataframe):\n",
    "#         \"\"\"\n",
    "#         Adds (or concatenates) a dataframe to the instance's master dataframe. The dataframe to be added must have atleast\n",
    "#         the following columns: station, latitude, longitude, featureStr, value\n",
    "#         \"\"\"\n",
    "\n",
    "#         self.sqlctxt.registerDataFrameAsTable(dataframe, \"temp\")\n",
    "#         query = f\"\"\"\n",
    "#         SELECT Station, latitude, longitude, COUNT(Measurement), MEAN(Values)\n",
    "#         FROM temp\n",
    "#         WHERE Measurement=='SNWD'\n",
    "#         GROUP BY Station, latitude, longitude\n",
    "#         \"\"\"\n",
    "#         tempdf = self.sqlctxt.sql(query).toPandas()\n",
    "        \n",
    "#         if (self.pdfMaster is None):\n",
    "#             # set as new pdfMaster\n",
    "#             self.pdfMaster = tempdf\n",
    "            \n",
    "#         else:            \n",
    "#             # append to existing dfMaster\n",
    "#             self.pdfMaster = pd.concat([self.pdfMaster, tempdf])\n",
    "            \n",
    "#     def plot_all(self):\n",
    "#         # update internals\n",
    "#         self.maxLong = self.pdfMaster['longitude'].max()\n",
    "#         self.minLong = self.pdfMaster['longitude'].min()\n",
    "#         self.maxLat = self.pdfMaster['latitude'].max()\n",
    "#         self.minLat = self.pdfMaster['latitude'].min()\n",
    "#         self.minAggVal = self.pdfMaster['avg(Values)'].min()\n",
    "#         self.maxAggVal = self.pdfMaster['avg(Values)'].max()\n",
    "        \n",
    "#         # update center of map\n",
    "#         self.center = [(self.minLat + self.maxLat)/2, (self.minLong + self.maxLong)/2]\n",
    "#         self.zoom = 6\n",
    "#         self.m = Map(default_tiles=TileLayer(opacity=1.0), center=self.center, zoom=self.zoom)\n",
    "        \n",
    "#         # loop over all the points in given dataframe, adding them to self.map\n",
    "#         circles = []\n",
    "#         for index,row in self.pdfMaster.iterrows():\n",
    "#             _lat=row['latitude']\n",
    "#             _long=row['longitude']\n",
    "#             _count=row['count(Measurement)']\n",
    "#             _coef=row['avg(Values)']\n",
    "# #             pdb.set_trace()\n",
    "#             # taking sqrt of count so that the  area of the circle corresponds to the count\n",
    "#             c = Circle(location=(_lat,_long), radius=int(1200*np.sqrt(_count+0.0)), weight=1,\n",
    "#                     color='#AAA', opacity=0.8, fill_opacity=0.4,\n",
    "#                     fill_color=self.get_color(_coef))\n",
    "#             circles.append(c)\n",
    "#             self.m.add_layer(c)\n",
    "#         self.m\n",
    "        \n",
    "    \n",
    "#     def color_legend(self):\n",
    "#         self.cfig = figure(figsize=[10,1])\n",
    "#         ax = plt.subplot(111)\n",
    "#         vals = self.cmap(np.arange(1,0,-.005))[:,:3]\n",
    "#         vals3 = np.stack([vals]*10)\n",
    "#         vals3.shape\n",
    "#         ax.imshow(vals3)\n",
    "#         midpoint = 200. * -self.minAggVal/(self.maxAggVal - self.minAggVal)\n",
    "#         xticks((0,midpoint,200),[\"%4.1f\"%v for v in (self.minAggVal,0.,self.maxAggVal)])\n",
    "#         yticks(());\n",
    "\n",
    "#     def get_color(self, val):\n",
    "#         x = (val - self.minAggVal)/(self.maxAggVal - self.minAggVal)\n",
    "#         return(rgb2hex(self.cmap(x)[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
