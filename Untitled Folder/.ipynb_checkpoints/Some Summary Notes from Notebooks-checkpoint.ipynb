{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Zen of Freund\n",
    "Summarizing selections the .ipynb of Yoav Freund from Public-DSC291/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spark Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### A more accurate picture of Spark Architecture\n",
    "\n",
    "In my search for a good description of Spark Architecture, I found [this](http://0x0fff.com/spark-architecture/) interesting Blog by Alexey Grishchenko.\n",
    "\n",
    "In particular I like the following figure. A few things to note:\n",
    "* Each executor lives in a separate JVM\n",
    "* A worker node might have several executors under it.\n",
    "* A partition defines a unit of work that resides in a single memory space, there is no 1-1 relationship between partitions and executors.  \n",
    "  \n",
    "    \n",
    "    \n",
    "![SparkArchitecture](http://0x0fff.com/wp-content/uploads/2015/03/Spark-Architecture-On-YARN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Take Home from '8. Glom.ipynb'\n",
    "* Partition your data and your computation in such a way that each partition holds the data needed to compute a part of the result.\n",
    "* Use a partitioner to bring related to each other to the same machine.\n",
    "* Distribute the load across all of the machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pipe command\n",
    "`RDD.pipe(cmd)` is a command that sends each element of the RDD as input to the command `cmd`.\n",
    "The command is any unix command line command, either pre-built, or defined by the programmer.\n",
    "\n",
    "The pipe() command allows the program to interface between spark and any other program. This is useful, in particular, when there is some legacy software, possibly written in `Matlab` or `Fortran` that we need to use, but that was not designed for parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A slick way to write and test an executable .py from a notebook interface\n",
    "http://localhost:8888/notebooks/Public-DSC291/notebooks/Section1-Basics/1.BasicSpark/Extra%20Notebooks/8.1.%20Pipe%20Demonstration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary from 2.SparkSQLDataFrameOperations\n",
    "\n",
    "* Dataframes can be manipulated decleratively, which allows for more optimization.\n",
    "* Dataframes can be stored and retrieved from Parquet files.\n",
    "* It is possible to refer directly to a parquet file in an SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* For an introduction to Spark SQL and Dataframes see: [Spark SQL, DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-dataframes-and-datasets-guide)\n",
    "* Also [spark-dataframe-and-operations](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/) from [analyticsvidhya.com](https://www.analyticsvidhya.com)\n",
    "\n",
    "For complete API reference see\n",
    "* [SQL programming guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) For Java, Scala and Python (Implementation is first in Scala and Python, later pyspark)\n",
    "* [pyspark API for the DataFrame class](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) \n",
    "* [pyspark API for the pyspark.sql module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T22:59:37.441127Z",
     "start_time": "2020-04-28T22:59:37.431926Z"
    }
   },
   "source": [
    "## organization of amazon web service's elastic map reduce (AWS-EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T23:01:31.443850Z",
     "start_time": "2020-04-28T23:01:31.438823Z"
    }
   },
   "source": [
    "<img alt=\"\" src=\"Figures/AWS-EMR-S3.png\" style=\"height:227.5px;width:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataframeStatFunctions\n",
    "\n",
    "Some methods for statistics functionality. \n",
    "\n",
    "* **approxQuantile(col, probabilities, relativeError)**\tCalculates the approximate quantiles of a numerical column of a DataFrame.\n",
    "* **corr(col1, col2[, method])**\tCalculates the correlation of two columns of a DataFrame as a double value.\n",
    "* **cov(col1, col2)**\tCalculate the sample covariance for the given columns, specified by their names, as a double value.\n",
    "* **crosstab(col1, col2)**\tComputes a pair-wise frequency table of the given columns.\n",
    "* **freqItems(cols[, support])**\tFinding frequent items for columns, possibly with false positives.\n",
    "* **sampleBy(col, fractions[, seed])**\tReturns a stratified sample without replacement based on the fraction given on each stratum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
